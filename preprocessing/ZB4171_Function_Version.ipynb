{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ZB4171 Function Version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsDl4GGZ716X",
        "outputId": "201c1a0c-49ba-4fd6-a596-0902520c24d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import stats"
      ],
      "outputs": [],
      "metadata": {
        "id": "C1Nw3TLg8AJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dedup_scar_cell (data_initial):\n",
        "  #filter scar created by universal repair machinism and scars not caused by dbs\n",
        "  data1 = data_initial[ ((data_initial['Presence']<=7) & (data_initial['Embryos']<=31)) | (data_initial['p']<0.01)]\n",
        "\n",
        "  # group the scars that are sequenced multiple times in a cell, drop duplicates\n",
        "  bool_dup = data1.duplicated(subset=['Cell','Scar'],keep=False)\n",
        "  not_bool_dup = [not elem for elem in bool_dup]\n",
        "  my_df = data1[not_bool_dup].sort_values(by='Cell')\n",
        "  df = my_df[['Library','Cell','Scar']]\n",
        "  # the actual scar-cell relation \n",
        "  df_dedup = df.drop_duplicates()\n",
        "  \n",
        "  return df_dedup"
      ],
      "outputs": [],
      "metadata": {
        "id": "7WklT2im8AC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def count_scar_in_cell (df_dedup):\n",
        "  scar_names = df_dedup.Scar.unique()\n",
        "  scar_count = len(df_dedup.Scar.unique())\n",
        "\n",
        "  # Creating a dictionary to store cell count for each scar \n",
        "  sc_count_dict = {}\n",
        "  for i in range(scar_count):\n",
        "    tempdf = df_dedup[df_dedup['Scar']==scar_names[i]]\n",
        "    sc_count_dict[scar_names[i]]=(len(tempdf))\n",
        "\n",
        "  return sc_count_dict"
      ],
      "outputs": [],
      "metadata": {
        "id": "K90T0QBP8AAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def scar_pairs (df_dedup):\n",
        "  scar_pair_df = pd.DataFrame(columns=['Scar1','Scar2']) \n",
        "  cell_names = df_dedup.Cell.unique()\n",
        "\n",
        "  for i in range(N):\n",
        "    # find all scars in current cell\n",
        "    temp = df_dedup[df_dedup['Cell']==cell_names[i]].sort_values(by='Scar')\n",
        "    temp = temp.reset_index()\n",
        "    if len(df_dedup[df_dedup['Cell']==cell_names[i]])>1:\n",
        "      for j in range(len(temp.Scar)):\n",
        "            for k in range(j+1,len(temp.Scar)):\n",
        "                new_row = {'Scar1':temp.Scar[j], 'Scar2':temp.Scar[k]}\n",
        "                scar_pair_df = scar_pair_df.append(new_row, ignore_index=True)\n",
        "  \n",
        "  scar_pair_df = scar_pair_df.value_counts().rename_axis(['Scar1','Scar2']).reset_index(name='Counts')\n",
        "\n",
        "\n",
        "  return scar_pair_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "QLIHCQF59mqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_sparse_scar (scar_pair_df):\n",
        "  sc_names_remove0 = pd.concat([scar_pair_df['Scar1'],scar_pair_df['Scar2']]).drop_duplicates()\n",
        "  return sc_names_remove0"
      ],
      "outputs": [],
      "metadata": {
        "id": "5EbE4xB1rv25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''\n",
        "def remove_false_edge (scar_pair_df,sc_count_dict,N):\n",
        "  rd = 0.1\n",
        "  D = rd * N\n",
        "\n",
        "  count_bad = 0\n",
        "  count_good = 0\n",
        "  scar_pair_tested = pd.DataFrame(columns=['Scar1','Scar2']) \n",
        "  for i in range(len(scar_pair_df)):\n",
        "    Nx = sc_count_dict[scar_pair_df.iloc[i]['Scar1']]\n",
        "    Ny = sc_count_dict[scar_pair_df.iloc[i]['Scar2']]\n",
        "\n",
        "    dxy = 2 *rd *Nx *Ny /N\n",
        "    rxy = 2 *rd *Nx *Ny /(N*N)\n",
        "\n",
        "    p = stats.binom_test(dxy, n=D, p=rxy, alternative='greater')\n",
        "    if (p<0.01):\n",
        "      new_row = {'Scar1':scar_pair_df.iloc[i]['Scar1'], 'Scar2':scar_pair_df.iloc[i]['Scar2']}\n",
        "      scar_pair_tested = scar_pair_tested.append(new_row, ignore_index=True)\n",
        "      count_good = count_good+1\n",
        "    if (p>=0.01):\n",
        "      count_bad = count_bad+1\n",
        "\n",
        "  return scar_pair_tested\n",
        "  '''\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef remove_false_edge (scar_pair_df,sc_count_dict,N):\\n  rd = 0.1\\n  D = rd * N\\n\\n  count_bad = 0\\n  count_good = 0\\n  scar_pair_tested = pd.DataFrame(columns=['Scar1','Scar2']) \\n  for i in range(len(scar_pair_df)):\\n    Nx = sc_count_dict[scar_pair_df.iloc[i]['Scar1']]\\n    Ny = sc_count_dict[scar_pair_df.iloc[i]['Scar2']]\\n\\n    dxy = 2 *rd *Nx *Ny /N\\n    rxy = 2 *rd *Nx *Ny /(N*N)\\n\\n    p = stats.binom_test(dxy, n=D, p=rxy, alternative='greater')\\n    if (p<0.01):\\n      new_row = {'Scar1':scar_pair_df.iloc[i]['Scar1'], 'Scar2':scar_pair_df.iloc[i]['Scar2']}\\n      scar_pair_tested = scar_pair_tested.append(new_row, ignore_index=True)\\n      count_good = count_good+1\\n    if (p>=0.01):\\n      count_bad = count_bad+1\\n\\n  return scar_pair_tested\\n  \""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "id": "JiWar9hu9mmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1ae00468-5074-48d2-dbb4-2fe48000da29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qeWu4PI5SyVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import statsmodels.stats.multitest as smt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "metadata": {
        "id": "dySOekxQTOQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89844774-7ce6-421b-d302-2fbc00fad8e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_false_edge (scar_pair_df,sc_count_dict,N):\n",
        "  rd = 0.1\n",
        "  D = rd * N\n",
        "\n",
        "  count_bad = 0\n",
        "  count_good = 0\n",
        "  scar_pair_tested = pd.DataFrame(columns=['Scar1','Scar2','p','p_adj']) \n",
        "  for i in range(len(scar_pair_df)):\n",
        "    Nx = sc_count_dict[scar_pair_df.iloc[i]['Scar1']]\n",
        "    Ny = sc_count_dict[scar_pair_df.iloc[i]['Scar2']]\n",
        "\n",
        "    dxy = 2 *rd *Nx *Ny /N\n",
        "    rxy = 2 *rd *Nx *Ny /(N*N)\n",
        "    Nxy = scar_pair_df.iloc[i]['Counts']\n",
        "\n",
        "    p = stats.binom_test(Nxy, n=N, p=rxy, alternative='greater')\n",
        "\n",
        "    new_row = {'Scar1':scar_pair_df.iloc[i]['Scar1'], 'Scar2':scar_pair_df.iloc[i]['Scar2'],'p':p}\n",
        "    scar_pair_tested = scar_pair_tested.append(new_row, ignore_index=True)\n",
        "\n",
        "  p_adj = smt.multipletests(scar_pair_tested['p'].to_list(), alpha=0.01, method='fdr_bh')\n",
        "  scar_pair_tested['p_adj'] = p_adj[1]\n",
        "  scar_pair_tested = scar_pair_tested[scar_pair_tested['p_adj']<0.01]\n",
        "  '''\n",
        "    if (p<=0.01):\n",
        "      new_row = {'Scar1':scar_pair_df.iloc[i]['Scar1'], 'Scar2':scar_pair_df.iloc[i]['Scar2']}\n",
        "      scar_pair_tested = scar_pair_tested.append(new_row, ignore_index=True)\n",
        "      count_good = count_good+1\n",
        "    if (p>0.01):\n",
        "      count_bad = count_bad+1\n",
        "  '''\n",
        "\n",
        "  return scar_pair_tested"
      ],
      "outputs": [],
      "metadata": {
        "id": "6KzItvSvnOTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def remove_false_edge_old (scar_pair_df,sc_count_dict,N):\n",
        "  rd = 0.1\n",
        "  D = rd * N\n",
        "\n",
        "  count_bad = 0\n",
        "  count_good = 0\n",
        "  scar_pair_tested = pd.DataFrame(columns=['Scar1','Scar2']) \n",
        "  for i in range(len(scar_pair_df)):\n",
        "    Nx = sc_count_dict[scar_pair_df.iloc[i]['Scar1']]\n",
        "    Ny = sc_count_dict[scar_pair_df.iloc[i]['Scar2']]\n",
        "\n",
        "    dxy = 2 *rd *Nx *Ny /N\n",
        "    rxy = 2 *rd *Nx *Ny /(N*N)\n",
        "    Nxy = scar_pair_df.iloc[i]['Counts']\n",
        "\n",
        "    p = stats.binom_test(Nxy, n=N, p=rxy, alternative='greater')\n",
        "    if (p<0.01):\n",
        "      new_row = {'Scar1':scar_pair_df.iloc[i]['Scar1'], 'Scar2':scar_pair_df.iloc[i]['Scar2']}\n",
        "      scar_pair_tested = scar_pair_tested.append(new_row, ignore_index=True)\n",
        "      count_good = count_good+1\n",
        "    if (p>=0.01):\n",
        "      count_bad = count_bad+1\n",
        "  \n",
        "\n",
        "  return scar_pair_tested\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "vGnMMAXIRqbD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def scar_degree (scar_pair_df,scar_names):\n",
        "  scar_connectivity = pd.DataFrame(columns=['Scar','Connectivity']) \n",
        "  for i in scar_names:\n",
        "    new_row = {'Scar': i, 'Connectivity': len(scar_pair_df[scar_pair_df['Scar1']==i])+len(scar_pair_df[scar_pair_df['Scar2']==i])} \n",
        "    scar_connectivity = scar_connectivity.append(new_row, ignore_index=True)\n",
        "\n",
        "  scar_connectivity.sort_values(by='Connectivity', ascending=False, inplace=True)\n",
        "  return scar_connectivity"
      ],
      "outputs": [],
      "metadata": {
        "id": "03stuub8_cad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def force_1c (scar_pair_df,scar_connectivity,scar_names):\n",
        "  top = scar_connectivity.iloc[0]['Scar']\n",
        "  scar_pair_df_1c = scar_pair_df\n",
        "  for i in scar_names:\n",
        "    if i != top:\n",
        "      if i<top:\n",
        "        #i=scar1,top=scar2\n",
        "        new_row = {'Scar1':i, 'Scar2':top}\n",
        "        scar_pair_df_1c = scar_pair_df_1c.append(new_row, ignore_index=True)\n",
        "      else: \n",
        "        new_row = {'Scar1':top, 'Scar2':i}\n",
        "        scar_pair_df_1c = scar_pair_df_1c.append(new_row, ignore_index=True)\n",
        "  \n",
        "  scar_pair_df_1c=scar_pair_df_1c.drop_duplicates()\n",
        "  return scar_pair_df_1c"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z68cRrUbzpeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def force_1c_0_as_Root (scar_pair_df,scar_connectivity,scar_names):\n",
        "  top = '0'\n",
        "  scar_pair_df_1c = scar_pair_df\n",
        "  for i in scar_names:\n",
        "      new_row = {'Scar1':top, 'Scar2':i}\n",
        "      scar_pair_df_1c = scar_pair_df_1c.append(new_row, ignore_index=True)\n",
        "  \n",
        "  scar_pair_df_1c=scar_pair_df_1c.drop_duplicates()\n",
        "  return scar_pair_df_1c"
      ],
      "outputs": [],
      "metadata": {
        "id": "pNOXoQrkEV9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def final_format (scar_pair_df_1c,sc_count_dict):\n",
        "  final_scar_pair_df = pd.DataFrame(columns=['Scar1','Scar2','cellCount1','cellCount2'])\n",
        "  for i in range(len(scar_pair_df_1c)):\n",
        "    scar1 = scar_pair_df_1c.iloc[i]['Scar1']\n",
        "    scar2 = scar_pair_df_1c.iloc[i]['Scar2']\n",
        "    if scar1=='0':\n",
        "      count1 = 10000\n",
        "    else:\n",
        "      count1 = sc_count_dict[scar1]\n",
        "    count2 = sc_count_dict[scar2]\n",
        "    new_row = {'Scar1': scar1, 'Scar2':scar2,'cellCount1': count1,'cellCount2':count2}\n",
        "    final_scar_pair_df = final_scar_pair_df.append(new_row, ignore_index=True)\n",
        "  return final_scar_pair_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "bPRAnyZJvdl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "s_u_-gEp9dzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "egXlM8uOLDBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**RUN**\n",
        "\n"
      ],
      "metadata": {
        "id": "qAXGL2kEvS4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "d5 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A5_scars_compared.csv\")\n",
        "d6 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A6_scars_compared.csv\")\n",
        "d7 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A7_scars_compared.csv\")\n",
        "data_initial = pd.concat([d5,d6,d7])\n",
        "#data_initial = d5\n",
        "#data_initial = d6\n",
        "#data_initial = d7"
      ],
      "outputs": [],
      "metadata": {
        "id": "TMjNO4Npygt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_initial = data_initial[(data_initial['Library']=='P7exo')|(data_initial['Library']=='P7endo')|(data_initial['Library']=='P5')|(data_initial['Library']=='P6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B7')|(data_initial['Library']=='B5')|(data_initial['Library']=='B6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='H7')|(data_initial['Library']=='H5')|(data_initial['Library']=='H6')]\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='H5')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B5')] ### No valid scar after testing \n",
        "#data_initial = data_initial[(data_initial['Library']=='P5')]\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='H6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B6')]. ### NO VALID SCAR even before testing\n",
        "#data_initial = data_initial[(data_initial['Library']=='P6')]\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='H7')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B7')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='P7exo')|(data_initial['Library']=='P7endo')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='P7endo')]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ptaWwdkoykgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Find out unique cell-scar combination\n",
        "df_dedup = dedup_scar_cell(data_initial)\n",
        "\n",
        "# total cell count (before filtering)\n",
        "N = len(df_dedup.Cell.unique())\n",
        "\n",
        "# cell count for each scar\n",
        "sc_count_dict = count_scar_in_cell (df_dedup)\n",
        "\n",
        "# scar1-scar2 pairs \n",
        "scar_pair_df = scar_pairs (df_dedup)\n",
        "\n",
        "# remove edges resulting from doublets \n",
        "scar_pair_tested = remove_false_edge (scar_pair_df,sc_count_dict,N)\n",
        "\n",
        "\n",
        "\n",
        "# remove sparse scar\n",
        "scar_names_remove0 = remove_sparse_scar (scar_pair_df)\n",
        "scar_names_tested_remove0 = remove_sparse_scar (scar_pair_tested)\n",
        "\n",
        "\n",
        "# calculate scar connectivity (to find the root scar)\n",
        "scar_connectivity = scar_degree (scar_pair_df,scar_names_remove0)\n",
        "scar_tested_connectivity = scar_degree (scar_pair_tested,scar_names_tested_remove0)\n",
        "\n",
        "\n",
        "# force connect to root node\n",
        "'''old\n",
        "scar_pair_df_1c = force_1c (scar_pair_df,scar_connectivity,scar_names_remove0)\n",
        "scar_pair_df_tested_1c = force_1c (scar_pair_tested,scar_tested_connectivity,scar_names_tested_remove0)\n",
        "'''\n",
        "\n",
        "scar_pair_df_1c = force_1c_0_as_Root (scar_pair_df,scar_connectivity,scar_names_remove0)\n",
        "scar_pair_df_tested_1c = force_1c_0_as_Root (scar_pair_tested,scar_tested_connectivity,scar_names_tested_remove0)\n",
        "\n",
        "\n",
        "# add cell count to df \n",
        "final_scar_pair_df = final_format (scar_pair_df_1c,sc_count_dict)\n",
        "final_scar_pair_tested_df = final_format (scar_pair_df_tested_1c,sc_count_dict)\n",
        "\n",
        "\n",
        "#scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/scar_pair_A567_Pancreas.csv\")\n",
        "#final_scar_pair_tested_df\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "thNwKLFM_ceN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version2/A567_H_all.csv\")\n",
        "final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version2/A567_P_tested.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "hvdoYyrHFFlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LiuShuiXian"
      ],
      "metadata": {
        "id": "WKuegvfyeN1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "d5 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A5_scars_compared.csv\")\n",
        "d6 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A6_scars_compared.csv\")\n",
        "d7 = pd.read_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/GSE106121_A7_scars_compared.csv\")\n",
        "#No need# data_initial = pd.concat([d5,d6,d7])\n",
        "#data_initial = d5\n",
        "data_initial = d6\n",
        "#data_initial = d7\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='P7exo')|(data_initial['Library']=='P7endo')|(data_initial['Library']=='P5')|(data_initial['Library']=='P6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B7')|(data_initial['Library']=='B5')|(data_initial['Library']=='B6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='H7')|(data_initial['Library']=='H5')|(data_initial['Library']=='H6')]\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='H5')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B5')] ### No valid scar after testing \n",
        "#data_initial = data_initial[(data_initial['Library']=='P5')]\n",
        "\n",
        "data_initial = data_initial[(data_initial['Library']=='H6')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B6')]. ### NO VALID SCAR even before testing\n",
        "#data_initial = data_initial[(data_initial['Library']=='P6')]\n",
        "\n",
        "#data_initial = data_initial[(data_initial['Library']=='H7')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='B7')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='P7exo')|(data_initial['Library']=='P7endo')]\n",
        "#data_initial = data_initial[(data_initial['Library']=='P7endo')]\n",
        "\n",
        "\n",
        "\n",
        "# Find out unique cell-scar combination\n",
        "df_dedup = dedup_scar_cell(data_initial)\n",
        "\n",
        "# total cell count (before filtering)\n",
        "N = len(df_dedup.Cell.unique())\n",
        "\n",
        "# cell count for each scar\n",
        "sc_count_dict = count_scar_in_cell (df_dedup)\n",
        "\n",
        "# scar1-scar2 pairs \n",
        "scar_pair_df = scar_pairs (df_dedup)\n",
        "\n",
        "# remove edges resulting from doublets \n",
        "scar_pair_tested = remove_false_edge (scar_pair_df,sc_count_dict,N)\n",
        "scar_pair_tested_old = remove_false_edge_old (scar_pair_df,sc_count_dict,N)\n",
        "\n",
        "\n",
        "# remove sparse scar\n",
        "scar_names_remove0 = remove_sparse_scar (scar_pair_df)\n",
        "scar_names_tested_remove0 = remove_sparse_scar (scar_pair_tested)\n",
        "\n",
        "\n",
        "# calculate scar connectivity (to find the root scar)\n",
        "scar_connectivity = scar_degree (scar_pair_df,scar_names_remove0)\n",
        "scar_tested_connectivity = scar_degree (scar_pair_tested,scar_names_tested_remove0)\n",
        "\n",
        "\n",
        "# force connect to root node\n",
        "'''old\n",
        "scar_pair_df_1c = force_1c (scar_pair_df,scar_connectivity,scar_names_remove0)\n",
        "scar_pair_df_tested_1c = force_1c (scar_pair_tested,scar_tested_connectivity,scar_names_tested_remove0)\n",
        "'''\n",
        "\n",
        "scar_pair_df_1c = force_1c_0_as_Root (scar_pair_df,scar_connectivity,scar_names_remove0)\n",
        "scar_pair_df_tested_1c = force_1c_0_as_Root (scar_pair_tested,scar_tested_connectivity,scar_names_tested_remove0)\n",
        "\n",
        "\n",
        "# add cell count to df \n",
        "final_scar_pair_df = final_format (scar_pair_df_1c,sc_count_dict)\n",
        "final_scar_pair_tested_df = final_format (scar_pair_df_tested_1c,sc_count_dict)\n",
        "\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version2/A7_H_all.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version2/A5_HPBtested.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version2/A5_H_tested_NEW.csv\")\n",
        "\n",
        "new = final_scar_pair_tested_df[(final_scar_pair_tested_df['cellCount1']>=20) & (final_scar_pair_tested_df['cellCount2']>=20)]\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version3/A5_P.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version3/test_A6_all.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "sVQf1XdweMn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/Version3/test.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "LioGCwOG51eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_H_total.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_H_tested.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A7_all_tested.csv\")\n",
        "\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_B_total.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_B_tested.csv\")\n",
        "\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_P_total.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A567_P_tested.csv\")\n",
        "\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A7_H_tested.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A7_P_tested.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A7_B.csv\")\n",
        "\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A6_H_tested.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A6_P.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A6_B.csv\") ## NO SCAR\n",
        "\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A5_H_tested.csv\")\n",
        "#final_scar_pair_tested_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A5_P_tested.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A5_P.csv\")\n",
        "#final_scar_pair_df.to_csv(\"/content/drive/My Drive/Year Four/ZB4171 Project/ScarPair_A5_B.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "upeDWQNq_L6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploration of Poisson binomial (Not used, exceed RAM)"
      ],
      "metadata": {
        "id": "gGLXLVLZyuiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "scar_names = scar_names_tested_remove0\n",
        "scar_connectivity = scar_tested_connectivity\n",
        "t_count = 0\n",
        "for i in scar_names:\n",
        "  t_count = t_count + sc_count_dict[i]"
      ],
      "outputs": [],
      "metadata": {
        "id": "z1wDHuGcyuHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in scar_names:\n",
        "  pij_list = []\n",
        "  for j in scar_names:\n",
        "    if i!=j:\n",
        "      pij = 1 - (1-(sc_count_dict[i]/t_count))**sc_count_dict[j]\n",
        "      pij_list = pij_list + [pij]\n",
        "  print(pij_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9999439993807856, 0.9999999986403519, 0.9999999927665613, 0.9999926426636504, 0.9991274850254814, 0.9999652633765148, 0.999989473729366, 0.9999097186474174]\n",
            "[0.9999361690636115, 0.9999998527030705, 0.9999994660226139, 0.9998891479529387, 0.995606333751805, 0.9996334604814452, 0.9998539169026597, 0.999234866046896]\n",
            "[0.9999999994142674, 0.9999999383386736, 0.9999999999999843, 0.9999999980263986, 0.999993508780007, 0.9999999725649237, 0.9999999963772409, 0.9999998614139386]\n",
            "[0.9999999960380578, 0.9999997256112154, 0.9999999999999792, 0.9999999880318321, 0.9999809968947736, 0.9999998686933668, 0.9999999791988454, 0.9999994266153717]\n",
            "[0.9999923288102809, 0.9998987671729823, 0.9999999953265933, 0.9999999775274745, 0.9986640813692199, 0.9999353663037905, 0.9999789475297863, 0.9998414436142908]\n",
            "[0.9989501485963278, 0.9952832518388517, 0.9999859170414892, 0.9999648539230626, 0.998446364629659, 0.9963678661519866, 0.9981100090124492, 0.9938747540298207]\n",
            "[0.9999610788654599, 0.9996401306496268, 0.9999999341884551, 0.9999997451542745, 0.9999304695737437, 0.996672606005231, 0.9999070670780817, 0.9994701684770781]\n",
            "[0.9999888159140518, 0.9998641083511434, 0.9999999913644056, 0.9999999605107545, 0.9999785445508543, 0.9983488556584335, 0.999911982703422, 0.9997901941896998]\n",
            "[0.9998955593771679, 0.9992220948968942, 0.9999996715687011, 0.9999988850331785, 0.9998236552124525, 0.9942059293306693, 0.9994513890086063, 0.9997708556178658]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLsouBMwI9CH",
        "outputId": "5bb07e0f-31bb-4744-dff6-fc74c423af25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "scar_dr = pd.DataFrame(columns=['Scar','N','Prob']) \n",
        "#pij_list = []\n",
        "for i in scar_names:\n",
        "  pij_list = []\n",
        "  for j in scar_names:\n",
        "    if i!=j:\n",
        "      pij = 1 - (1-(sc_count_dict[i]/t_count))**sc_count_dict[j]\n",
        "      pij_list = pij_list + [pij]\n",
        "  # perform poisson binomial test\n",
        "  pb = PoiBin(pij_list)\n",
        "  obs_deg = scar_connectivity[scar_connectivity['Scar']==i].iloc[0,1]\n",
        "  new_row = {'Scar': i, 'N': sc_count_dict[i], 'Prob':pb.cdf(obs_deg)} \n",
        "  scar_dr= scar_dr.append(new_row, ignore_index=True)\n",
        "\n",
        "scar_dr.sort_values(by='Prob', ascending=False)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Scar</th>\n",
              "      <th>N</th>\n",
              "      <th>Prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13:46M6D29M</td>\n",
              "      <td>171</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32:75M</td>\n",
              "      <td>157</td>\n",
              "      <td>1.961006e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1033:51M1I23M</td>\n",
              "      <td>105</td>\n",
              "      <td>8.975447e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28:44M9D31M</td>\n",
              "      <td>99</td>\n",
              "      <td>8.163485e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>361:50M7I18M</td>\n",
              "      <td>96</td>\n",
              "      <td>8.119333e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>779:34S41M</td>\n",
              "      <td>78</td>\n",
              "      <td>4.050579e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>44:44M3D31M</td>\n",
              "      <td>86</td>\n",
              "      <td>3.883612e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1202:49M26S</td>\n",
              "      <td>82</td>\n",
              "      <td>1.569925e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>624:43M1I4M3D27M</td>\n",
              "      <td>59</td>\n",
              "      <td>7.979728e-17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Scar    N          Prob\n",
              "2       13:46M6D29M  171  1.000000e+00\n",
              "3            32:75M  157  1.961006e-11\n",
              "0     1033:51M1I23M  105  8.975447e-16\n",
              "4       28:44M9D31M   99  8.163485e-16\n",
              "7      361:50M7I18M   96  8.119333e-16\n",
              "8        779:34S41M   78  4.050579e-16\n",
              "6       44:44M3D31M   86  3.883612e-16\n",
              "1       1202:49M26S   82  1.569925e-16\n",
              "5  624:43M1I4M3D27M   59  7.979728e-17"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "Nfa1w1b3FLWe",
        "outputId": "c3f8c4fd-9e1d-4eea-ae71-76977ff48f8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sequence</th>\n",
              "      <th>Barcode</th>\n",
              "      <th>Library</th>\n",
              "      <th>Cell</th>\n",
              "      <th>Reads</th>\n",
              "      <th>CIGAR</th>\n",
              "      <th>UMI</th>\n",
              "      <th>Presence</th>\n",
              "      <th>p</th>\n",
              "      <th>Embryos</th>\n",
              "      <th>Scar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Sequence, Barcode, Library, Cell, Reads, CIGAR, UMI, Presence, p, Embryos, Scar]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "hu_990lOSuGA",
        "outputId": "18015dc1-7fbf-485d-d2ea-eedccbf200d3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "scar_connectivity"
      ],
      "outputs": [],
      "metadata": {
        "id": "MLdP8Vc7JZBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sc_count_dict['44:44M3D31M']"
      ],
      "outputs": [],
      "metadata": {
        "id": "i1sY27TGJ2PT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sc_count_dict['361:50M7I18M']"
      ],
      "outputs": [],
      "metadata": {
        "id": "LwHbwLChJ7Xo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sc_count_dict['1033:51M1I23M']"
      ],
      "outputs": [],
      "metadata": {
        "id": "8XYGPz07KFp8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "### Pbinom \n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 29, 2016\n",
        "Module:\n",
        "    poibin - Poisson Binomial Distribution\n",
        "Author:\n",
        "    Mika Straka\n",
        "Description:\n",
        "    Implementation of the Poisson Binomial distribution for the sum of\n",
        "    independent and not identically distributed random variables as described\n",
        "    in the reference [Hong2013]_.\n",
        "    Implemented method:\n",
        "        * ``pmf``: probability mass function\n",
        "        * ``cdf``: cumulative distribution function\n",
        "        * ``pval``: p-value (1 - cdf)\n",
        "Usage:\n",
        "    Be ``p`` a list or  NumPy array of success probabilities for ``n``\n",
        "    non-identically distributed Bernoulli random variables.\n",
        "    Import the module and create an instance of the distribution with::\n",
        "        >>> from poibin import PoiBin\n",
        "        >>> pb = PoiBin(p)\n",
        "    Be ``x`` a list or NumPy array of different number of successes.\n",
        "    To obtain the:\n",
        "    * probability mass function of x, use::\n",
        "        >>> pb.pmf(x)\n",
        "    * cumulative distribution function of x, use::\n",
        "        >>> pb.cdf(x)\n",
        "    * p-values of x, use::\n",
        "        >>> pb.pval(x)\n",
        "    The functions are applied component-wise and a NumPy array of the same\n",
        "    length as ``x`` is returned.\n",
        "References:\n",
        ".. [Hong2013] Yili Hong, On computing the distribution function for the Poisson\n",
        "    binomial distribution,\n",
        "    Computational Statistics & Data Analysis, Volume 59, March 2013,\n",
        "    Pages 41-51, ISSN 0167-9473,\n",
        "    http://dx.doi.org/10.1016/j.csda.2012.10.006.\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PoiBin(object):\n",
        "    \"\"\"Poisson Binomial distribution for random variables.\n",
        "    This class implements the Poisson Binomial distribution for Bernoulli\n",
        "    trials with different success probabilities. The distribution describes\n",
        "    thus a random variable that is the sum of independent and not identically\n",
        "    distributed single Bernoulli random variables.\n",
        "    The class offers methods for calculating the probability mass function, the\n",
        "    cumulative distribution function, and p-values for right-sided testing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probabilities):\n",
        "        \"\"\"Initialize the class and calculate the ``pmf`` and ``cdf``.\n",
        "        :param probabilities: sequence of success probabilities :math:`p_i \\\\in\n",
        "            [0, 1] \\\\forall i \\\\in [0, N]` for :math:`N` independent but not\n",
        "            identically distributed Bernoulli random variables\n",
        "        :type probabilities: numpy.array\n",
        "        \"\"\"\n",
        "        self.success_probabilities = np.array(probabilities)\n",
        "        self.number_trials = self.success_probabilities.size\n",
        "        self.check_input_prob()\n",
        "        self.omega = 2 * np.pi / (self.number_trials + 1)\n",
        "        self.pmf_list = self.get_pmf_xi()\n",
        "        self.cdf_list = self.get_cdf(self.pmf_list)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Methods for the Poisson Binomial Distribution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "    def pmf(self, number_successes):\n",
        "        \"\"\"Calculate the probability mass function ``pmf`` for the input values.\n",
        "        The ``pmf`` is defined as\n",
        "        .. math::\n",
        "            pmf(k) = Pr(X = k), k = 0, 1, ..., n.\n",
        "        :param number_successes: number of successful trials for which the\n",
        "            probability mass function is calculated\n",
        "        :type number_successes: int or list of integers\n",
        "        \"\"\"\n",
        "        self.check_rv_input(number_successes)\n",
        "        return self.pmf_list[number_successes]\n",
        "\n",
        "    def cdf(self, number_successes):\n",
        "        \"\"\"Calculate the cumulative distribution function for the input values.\n",
        "        The cumulative distribution function ``cdf`` for a number ``k`` of\n",
        "        successes is defined as\n",
        "        .. math::\n",
        "            cdf(k) = Pr(X \\\\leq k), k = 0, 1, ..., n.\n",
        "        :param number_successes: number of successful trials for which the\n",
        "            cumulative distribution function is calculated\n",
        "        :type number_successes: int or list of integers\n",
        "        \"\"\"\n",
        "        self.check_rv_input(number_successes)\n",
        "        return self.cdf_list[number_successes]\n",
        "\n",
        "    def pval(self, number_successes):\n",
        "        \"\"\"Return the p-values corresponding to the input numbers of successes.\n",
        "        The p-values for right-sided testing are defined as\n",
        "        .. math::\n",
        "            pval(k) = Pr(X \\\\geq k ),  k = 0, 1, ..., n.\n",
        "        .. note::\n",
        "            Since :math:`cdf(k) = Pr(X <= k)`, the function returns\n",
        "            .. math::\n",
        "                1 - cdf(X < k) & = 1 - cdf(X <= k - 1)\n",
        "                               & = 1 - cdf(X <= k) + pmf(X = k),\n",
        "                               k = 0, 1, .., n.\n",
        "        :param number_successes: number of successful trials for which the\n",
        "            p-value is calculated\n",
        "        :type number_successes: int, numpy.array, or list of integers\n",
        "        \"\"\"\n",
        "        self.check_rv_input(number_successes)\n",
        "        i = 0\n",
        "        try:\n",
        "            isinstance(number_successes, collections.Iterable)\n",
        "            pvalues = np.array(number_successes, dtype='float')\n",
        "            # if input is iterable (list, numpy.array):\n",
        "            for k in number_successes:\n",
        "                pvalues[i] = 1. - self.cdf(k) + self.pmf(k)\n",
        "                i += 1\n",
        "            return pvalues\n",
        "        except TypeError:\n",
        "            # if input is an integer:\n",
        "            if number_successes == 0:\n",
        "                return 1\n",
        "            else:\n",
        "                return 1 - self.cdf(number_successes - 1)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Methods to obtain pmf and cdf\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "    def get_cdf(self, event_probabilities):\n",
        "        \"\"\"Return the values of the cumulative density function.\n",
        "        Return a list which contains all the values of the cumulative\n",
        "        density function for :math:`i = 0, 1, ..., n`.\n",
        "        :param event_probabilities: array of single event probabilities\n",
        "        :type event_probabilities: numpy.array\n",
        "        \"\"\"\n",
        "        cdf = np.empty(self.number_trials + 1)\n",
        "        cdf[0] = event_probabilities[0]\n",
        "        for i in range(1, self.number_trials + 1):\n",
        "            cdf[i] = cdf[i - 1] + event_probabilities[i]\n",
        "        return cdf\n",
        "\n",
        "    def get_pmf_xi(self):\n",
        "        \"\"\"Return the values of the variable ``xi``.\n",
        "        The components ``xi`` make up the probability mass function, i.e.\n",
        "        :math:`\\\\xi(k) = pmf(k) = Pr(X = k)`.\n",
        "        \"\"\"\n",
        "        chi = np.empty(self.number_trials + 1, dtype=complex)\n",
        "        chi[0] = 1\n",
        "        half_number_trials = int(\n",
        "            self.number_trials / 2 + self.number_trials % 2)\n",
        "        # set first half of chis:\n",
        "        chi[1:half_number_trials + 1] = self.get_chi(\n",
        "            np.arange(1, half_number_trials + 1))\n",
        "        # set second half of chis:\n",
        "        chi[half_number_trials + 1:self.number_trials + 1] = np.conjugate(\n",
        "            chi[1:self.number_trials - half_number_trials + 1] [::-1])\n",
        "        chi /= self.number_trials + 1\n",
        "        xi = np.fft.fft(chi)\n",
        "        if self.check_xi_are_real(xi):\n",
        "            xi = xi.real\n",
        "        else:\n",
        "            raise TypeError(\"pmf / xi values have to be real.\")\n",
        "        xi += np.finfo(type(xi[0])).eps\n",
        "        return xi\n",
        "\n",
        "    def get_chi(self, idx_array):\n",
        "        \"\"\"Return the values of ``chi`` for the specified indices.\n",
        "        :param idx_array: array of indices for which the ``chi`` values should\n",
        "            be calculated\n",
        "        :type idx_array: numpy.array\n",
        "        \"\"\"\n",
        "        # get_z:\n",
        "        exp_value = np.exp(self.omega * idx_array * 1j)\n",
        "        xy = 1 - self.success_probabilities + \\\n",
        "            self.success_probabilities * exp_value[:, np.newaxis]\n",
        "        # sum over the principal values of the arguments of z:\n",
        "        argz_sum = np.arctan2(xy.imag, xy.real).sum(axis=1)\n",
        "        # get d value:\n",
        "        exparg = np.log(np.abs(xy)).sum(axis=1)\n",
        "        d_value = np.exp(exparg)\n",
        "        # get chi values:\n",
        "        chi = d_value * np.exp(argz_sum * 1j)\n",
        "        return chi\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Auxiliary functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "    def check_rv_input(self, number_successes):\n",
        "        \"\"\"Assert that the input values ``number_successes`` are OK.\n",
        "        The input values ``number_successes`` for the random variable have to be\n",
        "        integers, greater or equal to 0, and smaller or equal to the total\n",
        "        number of trials ``self.number_trials``.\n",
        "        :param number_successes: number of successful trials\n",
        "        :type number_successes: int or list of integers \"\"\"\n",
        "        try:\n",
        "            for k in number_successes:\n",
        "                assert (type(k) == int or type(k) == np.int64), \\\n",
        "                        \"Values in input list must be integers\"\n",
        "                assert k >= 0, 'Values in input list cannot be negative.'\n",
        "                assert k <= self.number_trials, \\\n",
        "                    'Values in input list must be smaller or equal to the ' \\\n",
        "                    'number of input probabilities \"n\"'\n",
        "        except TypeError:\n",
        "            assert (type(number_successes) == int or \\\n",
        "                type(number_successes) == np.int64), \\\n",
        "                'Input value must be an integer.'\n",
        "            assert number_successes >= 0, \"Input value cannot be negative.\"\n",
        "            assert number_successes <= self.number_trials, \\\n",
        "                'Input value cannot be greater than ' + str(self.number_trials)\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def check_xi_are_real(xi_values):\n",
        "        \"\"\"Check whether all the ``xi``s have imaginary part equal to 0.\n",
        "        The probabilities :math:`\\\\xi(k) = pmf(k) = Pr(X = k)` have to be\n",
        "        positive and must have imaginary part equal to zero.\n",
        "        :param xi_values: single event probabilities\n",
        "        :type xi_values: complex\n",
        "        \"\"\"\n",
        "        return np.all(xi_values.imag <= np.finfo(float).eps)\n",
        "\n",
        "    def check_input_prob(self):\n",
        "        \"\"\"Check that all the input probabilities are in the interval [0, 1].\"\"\"\n",
        "        if self.success_probabilities.shape != (self.number_trials,):\n",
        "            raise ValueError(\n",
        "                \"Input must be an one-dimensional array or a list.\")\n",
        "        if not np.all(self.success_probabilities >= 0):\n",
        "            raise ValueError(\"Input probabilities have to be non negative.\")\n",
        "        if not np.all(self.success_probabilities <= 1):\n",
        "            raise ValueError(\"Input probabilities have to be smaller than 1.\")\n",
        "\n",
        "################################################################################\n",
        "# Main\n",
        "################################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "KVkYNTpgFHak"
      }
    }
  ]
}